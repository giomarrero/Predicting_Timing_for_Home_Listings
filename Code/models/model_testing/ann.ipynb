{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.12.0\n",
      "Num GPUs Available:  0\n",
      "Available GPU devices: []\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "\n",
    "tf.test.gpu_device_name()\n",
    "\n",
    "print(\"Available GPU devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='logs.log', level=logging.DEBUG, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "conn = sqlite3.connect('../data_eng/HOUSING.db')\n",
    "cursor = conn.cursor()\n",
    "df = pd.read_sql(\"select * from Realtor_Final_Merged\", conn)\n",
    "\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "df['target_bins'] = pd.qcut(df['median_days_on_market'], q=10, duplicates='drop')  \n",
    "\n",
    "train_set, temp_set = train_test_split(df, test_size=0.4, stratify=df['target_bins'], random_state=42)\n",
    "\n",
    "validation_set, test_set = train_test_split(temp_set, test_size=0.5, random_state=42)\n",
    "\n",
    "train_set = train_set.drop(columns=['target_bins','month_date_yyyymm','year'])\n",
    "validation_set = validation_set.drop(columns=['target_bins','month_date_yyyymm','year'])\n",
    "test_set = test_set.drop(columns=['target_bins','month_date_yyyymm','year'])\n",
    "\n",
    "logging.info(f\"Created Train Validate and Test sets\")\n",
    "\n",
    "train_set_x = train_set.drop('median_days_on_market', axis=1)\n",
    "train_set_y = train_set['median_days_on_market']\n",
    "\n",
    "\n",
    "val_set_x = validation_set.drop('median_days_on_market', axis=1)\n",
    "val_set_y = validation_set['median_days_on_market']\n",
    "\n",
    "test_set_x = test_set.drop('median_days_on_market', axis=1)\n",
    "test_set_y = test_set['median_days_on_market']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 00:26:23.685389: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38034/38034 [==============================] - 17s 441us/step - loss: 634112320.0000 - val_loss: 3458393.5000\n",
      "Epoch 2/100\n",
      "38034/38034 [==============================] - 17s 437us/step - loss: 7567715.0000 - val_loss: 29295.1113\n",
      "Epoch 3/100\n",
      "38034/38034 [==============================] - 17s 438us/step - loss: 9401.8896 - val_loss: 105518.2031\n",
      "Epoch 4/100\n",
      "38034/38034 [==============================] - 17s 438us/step - loss: 39388.3633 - val_loss: 5636.4053\n",
      "Epoch 5/100\n",
      "38034/38034 [==============================] - 17s 439us/step - loss: 3642.8708 - val_loss: 19503.2031\n",
      "Epoch 6/100\n",
      "38034/38034 [==============================] - 17s 438us/step - loss: 8242.3799 - val_loss: 20905.1875\n",
      "Epoch 7/100\n",
      "38034/38034 [==============================] - 17s 442us/step - loss: 8505.1221 - val_loss: 16538.8672\n",
      "Epoch 8/100\n",
      "38034/38034 [==============================] - 18s 470us/step - loss: 7355.0249 - val_loss: 2991.8630\n",
      "Epoch 9/100\n",
      "38034/38034 [==============================] - 17s 435us/step - loss: 2760.5544 - val_loss: 2657.6716\n",
      "Epoch 10/100\n",
      "38034/38034 [==============================] - 17s 434us/step - loss: 2653.7959 - val_loss: 2657.7688\n",
      "Epoch 11/100\n",
      "38034/38034 [==============================] - 17s 435us/step - loss: 2653.8953 - val_loss: 2657.6748\n",
      "Epoch 12/100\n",
      "38034/38034 [==============================] - 16s 434us/step - loss: 2653.8657 - val_loss: 2658.3259\n",
      "Epoch 13/100\n",
      "38034/38034 [==============================] - 16s 431us/step - loss: 2653.8005 - val_loss: 2657.9407\n",
      "Epoch 14/100\n",
      "38034/38034 [==============================] - 17s 436us/step - loss: 2653.8289 - val_loss: 2659.2698\n",
      "Epoch 15/100\n",
      "38034/38034 [==============================] - 21s 546us/step - loss: 2653.8801 - val_loss: 2657.6897\n",
      "Epoch 16/100\n",
      "38034/38034 [==============================] - 22s 582us/step - loss: 2653.8203 - val_loss: 2659.2810\n",
      "Epoch 17/100\n",
      "38034/38034 [==============================] - 16s 430us/step - loss: 2653.9204 - val_loss: 2657.7405\n",
      "Epoch 18/100\n",
      "38034/38034 [==============================] - 16s 432us/step - loss: 2653.7979 - val_loss: 2657.9067\n",
      "Epoch 19/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.8413 - val_loss: 2657.8271\n",
      "Epoch 20/100\n",
      "38034/38034 [==============================] - 16s 431us/step - loss: 2653.8054 - val_loss: 2657.5786\n",
      "Epoch 21/100\n",
      "38034/38034 [==============================] - 16s 431us/step - loss: 2653.8552 - val_loss: 2657.5786\n",
      "Epoch 22/100\n",
      "38034/38034 [==============================] - 16s 432us/step - loss: 2653.7922 - val_loss: 2657.7998\n",
      "Epoch 23/100\n",
      "38034/38034 [==============================] - 16s 430us/step - loss: 2653.8958 - val_loss: 2659.0022\n",
      "Epoch 24/100\n",
      "38034/38034 [==============================] - 16s 429us/step - loss: 2653.8340 - val_loss: 2658.4971\n",
      "Epoch 25/100\n",
      "38034/38034 [==============================] - 16s 430us/step - loss: 2653.8711 - val_loss: 2657.9670\n",
      "Epoch 26/100\n",
      "38034/38034 [==============================] - 16s 427us/step - loss: 2653.8486 - val_loss: 2659.1887\n",
      "Epoch 27/100\n",
      "38034/38034 [==============================] - 16s 427us/step - loss: 2653.8752 - val_loss: 2657.6824\n",
      "Epoch 28/100\n",
      "38034/38034 [==============================] - 16s 426us/step - loss: 2653.7759 - val_loss: 2658.2231\n",
      "Epoch 29/100\n",
      "38034/38034 [==============================] - 16s 428us/step - loss: 2653.7834 - val_loss: 2658.2827\n",
      "Epoch 30/100\n",
      "38034/38034 [==============================] - 16s 426us/step - loss: 2653.8792 - val_loss: 2657.8145\n",
      "Epoch 31/100\n",
      "38034/38034 [==============================] - 16s 427us/step - loss: 2653.8147 - val_loss: 2658.3848\n",
      "Epoch 32/100\n",
      "38034/38034 [==============================] - 16s 430us/step - loss: 2653.7781 - val_loss: 2657.5776\n",
      "Epoch 33/100\n",
      "38034/38034 [==============================] - 16s 428us/step - loss: 2653.8193 - val_loss: 2657.7312\n",
      "Epoch 34/100\n",
      "38034/38034 [==============================] - 16s 427us/step - loss: 2653.8340 - val_loss: 2657.7085\n",
      "Epoch 35/100\n",
      "38034/38034 [==============================] - 16s 428us/step - loss: 2653.8142 - val_loss: 2657.5708\n",
      "Epoch 36/100\n",
      "38034/38034 [==============================] - 16s 429us/step - loss: 2653.8086 - val_loss: 2657.9275\n",
      "Epoch 37/100\n",
      "38034/38034 [==============================] - 16s 429us/step - loss: 2653.7991 - val_loss: 2658.0479\n",
      "Epoch 38/100\n",
      "38034/38034 [==============================] - 16s 426us/step - loss: 2653.8035 - val_loss: 2657.6968\n",
      "Epoch 39/100\n",
      "38034/38034 [==============================] - 16s 427us/step - loss: 2653.8247 - val_loss: 2658.0474\n",
      "Epoch 40/100\n",
      "38034/38034 [==============================] - 16s 427us/step - loss: 2653.7717 - val_loss: 2657.9597\n",
      "Epoch 41/100\n",
      "38034/38034 [==============================] - 16s 427us/step - loss: 2653.8291 - val_loss: 2657.7324\n",
      "Epoch 42/100\n",
      "38034/38034 [==============================] - 16s 427us/step - loss: 2653.7971 - val_loss: 2657.7048\n",
      "Epoch 43/100\n",
      "38034/38034 [==============================] - 16s 426us/step - loss: 2653.7639 - val_loss: 2657.5745\n",
      "Epoch 44/100\n",
      "38034/38034 [==============================] - 16s 431us/step - loss: 2653.7964 - val_loss: 2657.9629\n",
      "Epoch 45/100\n",
      "38034/38034 [==============================] - 18s 472us/step - loss: 2653.7778 - val_loss: 2657.7764\n",
      "Epoch 46/100\n",
      "38034/38034 [==============================] - 16s 432us/step - loss: 2653.8533 - val_loss: 2657.6362\n",
      "Epoch 47/100\n",
      "38034/38034 [==============================] - 18s 474us/step - loss: 2653.7710 - val_loss: 2657.7646\n",
      "Epoch 48/100\n",
      "38034/38034 [==============================] - 62s 2ms/step - loss: 2653.8088 - val_loss: 2657.5757\n",
      "Epoch 49/100\n",
      "38034/38034 [==============================] - 16s 432us/step - loss: 2653.7742 - val_loss: 2657.6401\n",
      "Epoch 50/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.8005 - val_loss: 2657.6978\n",
      "Epoch 51/100\n",
      "38034/38034 [==============================] - 17s 435us/step - loss: 2653.7944 - val_loss: 2657.5874\n",
      "Epoch 52/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.8193 - val_loss: 2659.2222\n",
      "Epoch 53/100\n",
      "38034/38034 [==============================] - 17s 434us/step - loss: 2653.7896 - val_loss: 2658.8462\n",
      "Epoch 54/100\n",
      "38034/38034 [==============================] - 17s 443us/step - loss: 2653.7964 - val_loss: 2658.3000\n",
      "Epoch 55/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.7783 - val_loss: 2657.6338\n",
      "Epoch 56/100\n",
      "38034/38034 [==============================] - 17s 439us/step - loss: 2653.8196 - val_loss: 2657.6309\n",
      "Epoch 57/100\n",
      "38034/38034 [==============================] - 17s 436us/step - loss: 2653.7683 - val_loss: 2657.8721\n",
      "Epoch 58/100\n",
      "38034/38034 [==============================] - 17s 435us/step - loss: 2653.7939 - val_loss: 2657.9294\n",
      "Epoch 59/100\n",
      "38034/38034 [==============================] - 17s 439us/step - loss: 2653.8157 - val_loss: 2657.5801\n",
      "Epoch 60/100\n",
      "38034/38034 [==============================] - 17s 435us/step - loss: 2653.7595 - val_loss: 2657.6348\n",
      "Epoch 61/100\n",
      "38034/38034 [==============================] - 17s 434us/step - loss: 2653.7173 - val_loss: 2657.5798\n",
      "Epoch 62/100\n",
      "38034/38034 [==============================] - 17s 440us/step - loss: 2653.7864 - val_loss: 2657.9434\n",
      "Epoch 63/100\n",
      "38034/38034 [==============================] - 17s 434us/step - loss: 2653.7236 - val_loss: 2657.5972\n",
      "Epoch 64/100\n",
      "38034/38034 [==============================] - 17s 441us/step - loss: 2653.7595 - val_loss: 2658.2119\n",
      "Epoch 65/100\n",
      "38034/38034 [==============================] - 17s 444us/step - loss: 2653.7859 - val_loss: 2657.6790\n",
      "Epoch 66/100\n",
      "38034/38034 [==============================] - 17s 436us/step - loss: 2653.7546 - val_loss: 2657.8115\n",
      "Epoch 67/100\n",
      "38034/38034 [==============================] - 17s 438us/step - loss: 2653.7756 - val_loss: 2658.1204\n",
      "Epoch 68/100\n",
      "38034/38034 [==============================] - 17s 436us/step - loss: 2653.7058 - val_loss: 2657.6545\n",
      "Epoch 69/100\n",
      "38034/38034 [==============================] - 17s 442us/step - loss: 2653.7424 - val_loss: 2657.5745\n",
      "Epoch 70/100\n",
      "38034/38034 [==============================] - 17s 436us/step - loss: 2653.7300 - val_loss: 2657.5708\n",
      "Epoch 71/100\n",
      "38034/38034 [==============================] - 17s 438us/step - loss: 2653.7922 - val_loss: 2657.8682\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.6824 - val_loss: 2659.6963\n",
      "Epoch 73/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.7200 - val_loss: 2658.7251\n",
      "Epoch 74/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.6748 - val_loss: 2658.3679\n",
      "Epoch 75/100\n",
      "38034/38034 [==============================] - 16s 434us/step - loss: 2653.7217 - val_loss: 2657.6270\n",
      "Epoch 76/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.6960 - val_loss: 2657.6238\n",
      "Epoch 77/100\n",
      "38034/38034 [==============================] - 17s 437us/step - loss: 2653.7776 - val_loss: 2657.7310\n",
      "Epoch 78/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.7195 - val_loss: 2658.9128\n",
      "Epoch 79/100\n",
      "38034/38034 [==============================] - 16s 432us/step - loss: 2653.7808 - val_loss: 2657.5964\n",
      "Epoch 80/100\n",
      "38034/38034 [==============================] - 16s 434us/step - loss: 2653.7537 - val_loss: 2658.0554\n",
      "Epoch 81/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.7034 - val_loss: 2657.7463\n",
      "Epoch 82/100\n",
      "38034/38034 [==============================] - 17s 434us/step - loss: 2653.7483 - val_loss: 2657.7461\n",
      "Epoch 83/100\n",
      "38034/38034 [==============================] - 17s 447us/step - loss: 2653.7822 - val_loss: 2658.1445\n",
      "Epoch 84/100\n",
      "38034/38034 [==============================] - 17s 443us/step - loss: 2653.6992 - val_loss: 2657.5791\n",
      "Epoch 85/100\n",
      "38034/38034 [==============================] - 17s 436us/step - loss: 2653.7000 - val_loss: 2657.5969\n",
      "Epoch 86/100\n",
      "38034/38034 [==============================] - 17s 434us/step - loss: 2653.7217 - val_loss: 2660.1655\n",
      "Epoch 87/100\n",
      "38034/38034 [==============================] - 17s 437us/step - loss: 2653.7537 - val_loss: 2659.1372\n",
      "Epoch 88/100\n",
      "38034/38034 [==============================] - 17s 438us/step - loss: 2653.7778 - val_loss: 2657.5776\n",
      "Epoch 89/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.7427 - val_loss: 2658.2878\n",
      "Epoch 90/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.7554 - val_loss: 2657.5754\n",
      "Epoch 91/100\n",
      "38034/38034 [==============================] - 16s 432us/step - loss: 2653.7861 - val_loss: 2658.2710\n",
      "Epoch 92/100\n",
      "38034/38034 [==============================] - 17s 456us/step - loss: 2653.7744 - val_loss: 2658.3708\n",
      "Epoch 93/100\n",
      "38034/38034 [==============================] - 17s 437us/step - loss: 2653.7676 - val_loss: 2658.0857\n",
      "Epoch 94/100\n",
      "38034/38034 [==============================] - 17s 434us/step - loss: 2653.7292 - val_loss: 2657.6833\n",
      "Epoch 95/100\n",
      "38034/38034 [==============================] - 17s 435us/step - loss: 2653.7129 - val_loss: 2657.9519\n",
      "Epoch 96/100\n",
      "38034/38034 [==============================] - 17s 435us/step - loss: 2653.7131 - val_loss: 2657.9211\n",
      "Epoch 97/100\n",
      "38034/38034 [==============================] - 17s 439us/step - loss: 2653.7241 - val_loss: 2657.5757\n",
      "Epoch 98/100\n",
      "38034/38034 [==============================] - 17s 438us/step - loss: 2653.6909 - val_loss: 2659.9194\n",
      "Epoch 99/100\n",
      "38034/38034 [==============================] - 17s 434us/step - loss: 2653.7200 - val_loss: 2657.6028\n",
      "Epoch 100/100\n",
      "38034/38034 [==============================] - 16s 433us/step - loss: 2653.6980 - val_loss: 2658.6755\n",
      "12678/12678 [==============================] - 3s 245us/step - loss: 2658.6755\n",
      "12678/12678 [==============================] - 3s 243us/step - loss: 2691.9480\n",
      "Validation Loss: 2658.675537109375\n",
      "Test Loss: 2691.947998046875\n"
     ]
    }
   ],
   "source": [
    "#Non Scaled data\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(train_set_x.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear')) \n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(train_set_x, train_set_y, validation_data=(val_set_x, val_set_y), epochs=100, batch_size=32)\n",
    "\n",
    "\n",
    "val_loss = model.evaluate(val_set_x, val_set_y)\n",
    "test_loss = model.evaluate(test_set_x, test_set_y)\n",
    "\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir/ann_regression/tuner0.json\n",
      "Epoch 1/20\n",
      "38034/38034 [==============================] - 16s 421us/step - loss: 1446086912.0000 - val_loss: 251366240.0000\n",
      "Epoch 2/20\n",
      "38034/38034 [==============================] - 16s 422us/step - loss: 15076349.0000 - val_loss: 65552.1016\n",
      "Epoch 3/20\n",
      "38034/38034 [==============================] - 16s 423us/step - loss: 246817.2812 - val_loss: 561289.2500\n",
      "Epoch 4/20\n",
      "38034/38034 [==============================] - 16s 422us/step - loss: 156292.7031 - val_loss: 404751.4375\n",
      "Epoch 5/20\n",
      "38034/38034 [==============================] - 16s 423us/step - loss: 152602.6562 - val_loss: 391499.8438\n",
      "Epoch 6/20\n",
      "38034/38034 [==============================] - 16s 424us/step - loss: 162702.3594 - val_loss: 554630.6875\n",
      "Epoch 7/20\n",
      "38034/38034 [==============================] - 16s 423us/step - loss: 284626.5625 - val_loss: 70476.4922\n",
      "Epoch 8/20\n",
      "38034/38034 [==============================] - 16s 421us/step - loss: 3443.7158 - val_loss: 530067.3125\n",
      "Epoch 9/20\n",
      "38034/38034 [==============================] - 16s 423us/step - loss: 136563.0000 - val_loss: 31182.6309\n",
      "Epoch 10/20\n",
      "38034/38034 [==============================] - 16s 422us/step - loss: 7011.6724 - val_loss: 399822.3750\n",
      "Epoch 11/20\n",
      "38034/38034 [==============================] - 16s 424us/step - loss: 302636.9062 - val_loss: 28435.6074\n",
      "Epoch 12/20\n",
      "38034/38034 [==============================] - 16s 424us/step - loss: 13321.5615 - val_loss: 593709.4375\n",
      "Epoch 13/20\n",
      "38034/38034 [==============================] - 16s 425us/step - loss: 245514.2188 - val_loss: 28435.5840\n",
      "Epoch 14/20\n",
      "38034/38034 [==============================] - 16s 423us/step - loss: 6166.0015 - val_loss: 341963.0938\n",
      "Epoch 15/20\n",
      "38034/38034 [==============================] - 16s 424us/step - loss: 142357.7656 - val_loss: 9608.2764\n",
      "Epoch 16/20\n",
      "38034/38034 [==============================] - 16s 425us/step - loss: 5494.3965 - val_loss: 341963.1562\n",
      "Epoch 17/20\n",
      "38034/38034 [==============================] - 16s 423us/step - loss: 36448.0820 - val_loss: 6751.5269\n",
      "Epoch 18/20\n",
      "38034/38034 [==============================] - 16s 426us/step - loss: 4375.6504 - val_loss: 278552.5938\n",
      "Epoch 19/20\n",
      "38034/38034 [==============================] - 17s 440us/step - loss: 13728.0479 - val_loss: 14988.9082\n",
      "Epoch 20/20\n",
      "38034/38034 [==============================] - 16s 430us/step - loss: 7711.2134 - val_loss: 141218.3750\n",
      "12678/12678 [==============================] - 3s 274us/step - loss: 59019.7383\n",
      "Test Loss: 59019.73828125\n"
     ]
    }
   ],
   "source": [
    "def build_model_nonscale(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), \n",
    "                           activation='relu', input_shape=(train_set_x.shape[1],)))\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1, activation='linear'))  # Output layer for regression\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model_nonscale,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='ann_regression'\n",
    ")\n",
    "\n",
    "tuner.search(train_set_x, train_set_y, epochs=20, validation_data=(val_set_x, val_set_y))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "best_model = build_model_nonscale(best_hps)\n",
    "\n",
    "# Fit the best model\n",
    "history = best_model.fit(train_set_x, train_set_y, epochs=20, validation_data=(val_set_x, val_set_y))\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss = best_model.evaluate(test_set_x, test_set_y)\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12678/12678 [==============================] - 3s 236us/step\n",
      "Mean Square Error for vanilla ANN Model - Non Scaled data\n",
      "59019.77014303767\n"
     ]
    }
   ],
   "source": [
    "pred_nonscale = best_model.predict(test_set_x)\n",
    "\n",
    "mse = mean_squared_error(test_set_y, pred_nonscale)\n",
    "print(\"Mean Square Error for vanilla ANN Model - Non Scaled data\")\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(test_set_x)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#train_set_y = train_set_y.reset_index()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#val_set_y = val_set_y.reset_index()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#test_set_y = test_set_y.reset_index()\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m train_set_y \u001b[38;5;241m=\u001b[39m np(train_set_y)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m val_set_y \u001b[38;5;241m=\u001b[39m val_set_y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m test_set_y \u001b[38;5;241m=\u001b[39m test_set_y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "#scale all the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_set_x)\n",
    "X_val_scaled = scaler.transform(val_set_x)\n",
    "X_test_scaled = scaler.transform(test_set_x)\n",
    "\n",
    "#train_set_y = train_set_y.reset_index()\n",
    "#val_set_y = val_set_y.reset_index()\n",
    "#test_set_y = test_set_y.reset_index()\n",
    "\n",
    "train_set_y = train_set_y.reshape(-1, 1)\n",
    "val_set_y = val_set_y.reshape(-1, 1)\n",
    "test_set_y = test_set_y.reshape(-1, 1)\n",
    "\n",
    "Y_train_scaled = scaler.fit_transform(train_set_y)\n",
    "Y_val_scaled = scaler.transform(val_set_y)\n",
    "Y_test_scaled = scaler.transform(test_set_y)\n",
    "\n",
    "Y_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38034/38034 [==============================] - 20s 516us/step - loss: 0.6037 - val_loss: 0.6025\n",
      "Epoch 2/100\n",
      "11679/38034 [========>.....................] - ETA: 11s - loss: 0.5945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#fit data \u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, Y_train_scaled, validation_data\u001b[38;5;241m=\u001b[39m(X_val_scaled, Y_val_scaled), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#evaluate using MSE as loss function\u001b[39;00m\n\u001b[1;32m     27\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val_scaled, Y_val_scaled)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:418\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    409\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m functional_ops\u001b[38;5;241m.\u001b[39mpartitioned_call(\n\u001b[1;32m    410\u001b[0m             args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    411\u001b[0m             f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m             config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    415\u001b[0m             executor_type\u001b[38;5;241m=\u001b[39mexecutor_type)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, func_graph_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func_graph_outputs):\n\u001b[0;32m--> 418\u001b[0m   handle_data_util\u001b[38;5;241m.\u001b[39mcopy_handle_data(func_graph_output, outputs[i])\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executing_eagerly:\n\u001b[1;32m    420\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#instantiate model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear')) \n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#fit data \n",
    "history = model.fit(X_train_scaled, Y_train_scaled, validation_data=(X_val_scaled, Y_val_scaled), epochs=100, batch_size=32)\n",
    "\n",
    "#evaluate using MSE as loss function\n",
    "val_loss = model.evaluate(X_val_scaled, Y_val_scaled)\n",
    "test_loss = model.evaluate(X_test_scaled, Y_test_scaled)\n",
    "\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Test Loss:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), \n",
    "                           activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1, activation='linear'))  # Output layer for regression\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='ann_regression'\n",
    ")\n",
    "\n",
    "tuner.search(X_train_scaled, Y_train_scaled, epochs=20, validation_data=(X_val_scaled, Y_val_scaled))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "best_model = build_model(best_hps)\n",
    "\n",
    "# Fit the best model\n",
    "history = best_model.fit(X_train_scaled, Y_train_scaled, epochs=20, validation_data=(X_val_scaled, Y_val_scaled))\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss = best_model.evaluate(X_test_scaled, Y_test_scaled)\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "mse = mean_squared_error(test_set_y, pred)\n",
    "print(\"Mean Square Error for vanilla ANN Model - scaled data\")\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model.save('ann_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ann_mse=mse \n",
    "#rf_mse=1808.0038216640519\n",
    "#reg_mse=2261.9921642924583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "#MSEs = [ann_mse, rf_mse, reg_mse]\n",
    "#model_labels = ['ANN MSE', 'Random Forest MSE', 'Regression MSE']\n",
    "\n",
    "#ax.bar(model_labels ,MSEs, label=model_labels)\n",
    "\n",
    "#ax.set_ylabel('MSE')\n",
    "#ax.set_title('Model MSE Comparison')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
